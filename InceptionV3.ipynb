{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of InceptionV3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hieu-le-2412/ai-course/blob/master/InceptionV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh0VWS5V9op9",
        "colab_type": "text"
      },
      "source": [
        "In this exercise, you will build a CNN model to classify: horse vs human.\n",
        "\n",
        "Firstly, you need to run the following code to download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RXZT2UsyIVe_",
        "outputId": "000bf5c8-4aae-4181-bf4b-619e47535349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
        "    -O /tmp/horse-or-human.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-19 04:04:14--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.69.128, 2607:f8b0:4001:c01::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.69.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149574867 (143M) [application/zip]\n",
            "Saving to: ‘/tmp/horse-or-human.zip’\n",
            "\n",
            "/tmp/horse-or-human 100%[===================>] 142.65M   189MB/s    in 0.8s    \n",
            "\n",
            "2019-06-19 04:04:15 (189 MB/s) - ‘/tmp/horse-or-human.zip’ saved [149574867/149574867]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mLij6qde6Ox",
        "colab_type": "code",
        "outputId": "a75353a0-bced-4db8-f027-cc3b358f7fdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip \\\n",
        "    -O /tmp/validation-horse-or-human.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-19 04:04:17--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.214.128, 2607:f8b0:4001:c00::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.214.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11480187 (11M) [application/zip]\n",
            "Saving to: ‘/tmp/validation-horse-or-human.zip’\n",
            "\n",
            "\r          /tmp/vali   0%[                    ]       0  --.-KB/s               \r/tmp/validation-hor 100%[===================>]  10.95M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2019-06-19 04:04:17 (120 MB/s) - ‘/tmp/validation-horse-or-human.zip’ saved [11480187/11480187]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9brUxyTpYZHy",
        "colab_type": "text"
      },
      "source": [
        "The following python code will use the OS library to use Operating System libraries, giving you access to the file system, and the zipfile library allowing you to unzip the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PLy3pthUS0D2",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/horse-or-human')\n",
        "local_zip = '/tmp/validation-horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/validation-horse-or-human')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o-qUPyfO7Qr8"
      },
      "source": [
        "The contents of the .zip are extracted to the base directory `/tmp/horse-or-human`, which in turn each contain `horses` and `humans` subdirectories.\n",
        "\n",
        "In short: The training set is the data that is used to tell the neural network model that 'this is what a horse looks like', 'this is what a human looks like' etc. \n",
        "\n",
        "One thing to pay attention to in this sample: We do not explicitly label the images as horses or humans. If you remember with the handwriting example earlier, we had labelled 'this is a 1', 'this is a 7' etc.  Later you'll see something called an ImageGenerator being used -- and this is coded to read images from subdirectories, and automatically label them from the name of that subdirectory. So, for example, you will have a 'training' directory containing a 'horses' directory and a 'humans' one. ImageGenerator will label the images appropriately for you, reducing a coding step. \n",
        "\n",
        "Let's define each of these directories:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NR_M9nWN-K8B",
        "colab": {}
      },
      "source": [
        "# Directory with our training horse pictures\n",
        "train_horse_dir = os.path.join('/tmp/horse-or-human/horses')\n",
        "\n",
        "# Directory with our training human pictures\n",
        "train_human_dir = os.path.join('/tmp/horse-or-human/humans')\n",
        "\n",
        "# Directory with our training horse pictures\n",
        "validation_horse_dir = os.path.join('/tmp/validation-horse-or-human/validation-horses')\n",
        "\n",
        "# Directory with our training human pictures\n",
        "validation_human_dir = os.path.join('/tmp/validation-horse-or-human/validation-humans')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Sn9m9D3UimHM"
      },
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "Let's set up data generators that will read pictures in our source folders, convert them to `float32` tensors, and feed them (with their labels) to our network. We'll have one generator for the training images and one for the validation images. Our generators will yield batches of images of size 300x300 and their labels (binary).\n",
        "\n",
        "As you may already know, data that goes into neural networks should usually be normalized in some way to make it more amenable to processing by the network. (It is uncommon to feed raw pixels into a convnet.) In our case, we will preprocess our images by normalizing the pixel values to be in the `[0, 1]` range (originally all values are in the `[0, 255]` range).\n",
        "\n",
        "In Keras this can be done via the `keras.preprocessing.image.ImageDataGenerator` class using the `rescale` parameter. This `ImageDataGenerator` class allows you to instantiate generators of augmented image batches (and their labels) via `.flow(data, labels)` or `.flow_from_directory(directory)`. These generators can then be used with the Keras model methods that accept data generators as inputs: `fit_generator`, `evaluate_generator`, and `predict_generator`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ClebU9NJg99G",
        "outputId": "b6e77364-26cd-4501-f53c-b96e7e0b7ed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "IMG_WIDTH, IMG_HEIGHT = 299,299\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/tmp/horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(IMG_WIDTH, IMG_HEIGHT),  # All images will be resized to 150x150\n",
        "        batch_size=128,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        '/tmp/validation-horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(IMG_WIDTH, IMG_HEIGHT),  # All images will be resized to 150x150\n",
        "        batch_size=32,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5oqBkNBJmtUv"
      },
      "source": [
        "## Building a Small Model from Scratch\n",
        "\n",
        "But before we continue, let's start defining the model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BnhYCP4tdqjC"
      },
      "source": [
        "\n",
        "\n",
        "**YOUR TASK**: Construct the  model containing the following layers:\n",
        "1. A convolution layer: 16 filters of 3x3, activation = relu\n",
        "2. A max pooling layer: filter of 2x2, stride = 2\n",
        "3. A convolution layer: 32 filters of 3x3, activation = relu\n",
        "4. A max pooling layer: filter of 2x2, stride = 2\n",
        "5. A convolution layer: 64 filters of 3x3, activation = relu\n",
        "6. A max pooling layer: filter of 2x2, stride = 2\n",
        "7. A flatten layer\n",
        "8. A dense layer: 512 neurons, activation = relu\n",
        "9. A dense layer: 1 neurons, activation = sigmoid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gokG5HKpdtzm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Note that because we are facing a two-class classification problem, i.e. a *binary classification problem*, we will end our network with a [*sigmoid* activation](https://wikipedia.org/wiki/Sigmoid_function), so that the output of our network will be a single scalar between 0 and 1, encoding the probability that the current image is class 1 (as opposed to class 0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PixZ2s5QbYQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVHzpNJZreUL",
        "colab_type": "code",
        "outputId": "e326b749-8f9b-4cbd-9cad-e88451df1ae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5304
        }
      },
      "source": [
        "# Create model of InceptionV3\n",
        "from keras.applications import InceptionV3;\n",
        "from keras import callbacks\n",
        "\n",
        "#Get back the convolutional part of a ResNet50 network trained on ImageNet\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(IMG_WIDTH,IMG_HEIGHT,3))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "#Add the fully-connected layers \n",
        "model.add(Flatten(name='flatten'))\n",
        "model.add(Dense(512, activation='relu', name='fc1'))\n",
        "model.add(Dense(1, activation='sigmoid', name='predictions'))\n",
        "\n",
        "# Freeze the layers \n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        " \n",
        "# Check the trainable status of the individual layers\n",
        "for layer in base_model.layers:\n",
        "    print(layer, layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<keras.engine.input_layer.InputLayer object at 0x7fa35653b6d8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa35653ba20> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa35653b668> False\n",
            "<keras.layers.core.Activation object at 0x7fa35653bdd8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa38523cf60> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa385217f28> False\n",
            "<keras.layers.core.Activation object at 0x7fa3851a08d0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3850ec908> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa3850cbfd0> False\n",
            "<keras.layers.core.Activation object at 0x7fa38508b8d0> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fa384fd2748> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa385004438> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa384f6f128> False\n",
            "<keras.layers.core.Activation object at 0x7fa384f6ff28> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa384ec1860> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa384f1cf60> False\n",
            "<keras.layers.core.Activation object at 0x7fa384ede828> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fa384e0b240> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa384b80f98> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa384b762b0> False\n",
            "<keras.layers.core.Activation object at 0x7fa384ba0b00> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa384d157b8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa384aedef0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa384cc8c88> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa384a88b70> False\n",
            "<keras.layers.core.Activation object at 0x7fa384c85588> False\n",
            "<keras.layers.core.Activation object at 0x7fa384a64e80> False\n",
            "<keras.layers.pooling.AveragePooling2D object at 0x7fa384893dd8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa384e5b390> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa384c64f28> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3849dbda0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3848c3ac8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa384d47080> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa384c37da0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa3849f5c88> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa3848b8e10> False\n",
            "<keras.layers.core.Activation object at 0x7fa384d47fd0> False\n",
            "<keras.layers.core.Activation object at 0x7fa384c4df28> False\n",
            "<keras.layers.core.Activation object at 0x7fa38494df60> False\n",
            "<keras.layers.core.Activation object at 0x7fa384805f28> False\n",
            "<keras.layers.merge.Concatenate object at 0x7fa38477fe48> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3844c8f98> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa384528470> False\n",
            "<keras.layers.core.Activation object at 0x7fa3844b8fd0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa384720400> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa384435ac8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa3846ce5f8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa384416f60> False\n",
            "<keras.layers.core.Activation object at 0x7fa38468f978> False\n",
            "<keras.layers.core.Activation object at 0x7fa3843d8828> False\n",
            "<keras.layers.pooling.AveragePooling2D object at 0x7fa358d8d7f0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa384836be0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3845d7d30> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa38431e908> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa358dbbd68> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa3847a3ef0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa38463b940> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa384300fd0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa358daae10> False\n",
            "<keras.layers.core.Activation object at 0x7fa3847a3c50> False\n",
            "<keras.layers.core.Activation object at 0x7fa3845faba8> False\n",
            "<keras.layers.core.Activation object at 0x7fa3842c0898> False\n",
            "<keras.layers.core.Activation object at 0x7fa358d7a668> False\n",
            "<keras.layers.merge.Concatenate object at 0x7fa358cf8860> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa358a1b240> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa358a3a668> False\n",
            "<keras.layers.core.Activation object at 0x7fa3589aa518> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa358be0ac8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa35890af60> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa358bbff60> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa3588dcdd8> False\n",
            "<keras.layers.core.Activation object at 0x7fa358b50dd8> False\n",
            "<keras.layers.core.Activation object at 0x7fa358977e80> False\n",
            "<keras.layers.pooling.AveragePooling2D object at 0x7fa358760da0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa358d26e48> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa358acf7b8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa358896f98> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa358746da0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa358c686d8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa358a7fd30> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa35885e860> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa3587aca20> False\n",
            "<keras.layers.core.Activation object at 0x7fa358c977f0> False\n",
            "<keras.layers.core.Activation object at 0x7fa358aee748> False\n",
            "<keras.layers.core.Activation object at 0x7fa358816b00> False\n",
            "<keras.layers.core.Activation object at 0x7fa3587acf60> False\n",
            "<keras.layers.merge.Concatenate object at 0x7fa3586fcf60> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa357d7da90> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa357d732b0> False\n",
            "<keras.layers.core.Activation object at 0x7fa357cfe978> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa357c86d68> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa357c3f1d0> False\n",
            "<keras.layers.core.Activation object at 0x7fa357c3f9b0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3586b3e10> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa357bbde48> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa358692048> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa357bd6be0> False\n",
            "<keras.layers.core.Activation object at 0x7fa357de4ac8> False\n",
            "<keras.layers.core.Activation object at 0x7fa357bb0ef0> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fa357b25e10> False\n",
            "<keras.layers.merge.Concatenate object at 0x7fa357b25ef0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa357723dd8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa35770d5c0> False\n",
            "<keras.layers.core.Activation object at 0x7fa357698f28> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa357616e80> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa357677358> False\n",
            "<keras.layers.core.Activation object at 0x7fa357631978> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3579c7e48> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa357500b00> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa3579a23c8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa357561f98> False\n",
            "<keras.layers.core.Activation object at 0x7fa357a2fac8> False\n",
            "<keras.layers.core.Activation object at 0x7fa357523898> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3578feef0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa357471940> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa357919b70> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa35744bf98> False\n",
            "<keras.layers.core.Activation object at 0x7fa3578f2e80> False\n",
            "<keras.layers.core.Activation object at 0x7fa35740d7f0> False\n",
            "<keras.layers.pooling.AveragePooling2D object at 0x7fa357246710> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa357adcb38> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa35783af60> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa357359828> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3572f4400> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa357a98ef0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa35781feb8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa35730bda0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa3572617f0> False\n",
            "<keras.layers.core.Activation object at 0x7fa357a11be0> False\n",
            "<keras.layers.core.Activation object at 0x7fa3577ddf60> False\n",
            "<keras.layers.core.Activation object at 0x7fa3573767b8> False\n",
            "<keras.layers.core.Activation object at 0x7fa35722c588> False\n",
            "<keras.layers.merge.Concatenate object at 0x7fa357192320> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa356dc5f28> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa356de6b38> False\n",
            "<keras.layers.core.Activation object at 0x7fa356d96e48> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa356d55fd0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa356cf72e8> False\n",
            "<keras.layers.core.Activation object at 0x7fa356cd51d0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3570a3940> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa356c50f60> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa35707df98> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa356c6ce80> False\n",
            "<keras.layers.core.Activation object at 0x7fa35700c7b8> False\n",
            "<keras.layers.core.Activation object at 0x7fa356bbf128> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa356fea278> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa356bbbe80> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa356f8c6a0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa356b59b38> False\n",
            "<keras.layers.core.Activation object at 0x7fa356fa97b8> False\n",
            "<keras.layers.core.Activation object at 0x7fa356b30978> False\n",
            "<keras.layers.pooling.AveragePooling2D object at 0x7fa356964d68> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa35717dd30> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa356eda160> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa356a78f28> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa356999a90> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa3571215f8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa356ec4ac8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa356a5b6a0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa356907ba8> False\n",
            "<keras.layers.core.Activation object at 0x7fa3571520b8> False\n",
            "<keras.layers.core.Activation object at 0x7fa356e976d8> False\n",
            "<keras.layers.core.Activation object at 0x7fa356a1deb8> False\n",
            "<keras.layers.core.Activation object at 0x7fa3568d7f98> False\n",
            "<keras.layers.merge.Concatenate object at 0x7fa35684fe10> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa356472940> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa3564096d8> False\n",
            "<keras.layers.core.Activation object at 0x7fa3563af9e8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa356331908> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa356311fd0> False\n",
            "<keras.layers.core.Activation object at 0x7fa3562d1898> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3567f19e8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa35621d7f0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa35678ab70> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa3561ced68> False\n",
            "<keras.layers.core.Activation object at 0x7fa356761e80> False\n",
            "<keras.layers.core.Activation object at 0x7fa3561ba780> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3566a8da0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa356168278> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa35668e588> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa3561086d8> False\n",
            "<keras.layers.core.Activation object at 0x7fa35664ddd8> False\n",
            "<keras.layers.core.Activation object at 0x7fa3561277f0> False\n",
            "<keras.layers.pooling.AveragePooling2D object at 0x7fa355f43f28> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa356884b38> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3564d6f98> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa356056160> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa355f8f048> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa356875e48> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa356537470> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa356040f98> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa355f119e8> False\n",
            "<keras.layers.core.Activation object at 0x7fa3567c4f98> False\n",
            "<keras.layers.core.Activation object at 0x7fa3564ef5c0> False\n",
            "<keras.layers.core.Activation object at 0x7fa3560146d8> False\n",
            "<keras.layers.core.Activation object at 0x7fa355ed1860> False\n",
            "<keras.layers.merge.Concatenate object at 0x7fa355eabfd0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa355ac6e10> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa355a807f0> False\n",
            "<keras.layers.core.Activation object at 0x7fa355b19a90> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa355a54eb8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa355a075c0> False\n",
            "<keras.layers.core.Activation object at 0x7fa3559c6f60> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa355de6d68> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa355909fd0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa355d6cc50> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa35596da58> False\n",
            "<keras.layers.core.Activation object at 0x7fa355d2b588> False\n",
            "<keras.layers.core.Activation object at 0x7fa355928cc0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa355cd3b00> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa355871da0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa355c49320> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa355856588> False\n",
            "<keras.layers.core.Activation object at 0x7fa355c15f98> False\n",
            "<keras.layers.core.Activation object at 0x7fa355812dd8> False\n",
            "<keras.layers.pooling.AveragePooling2D object at 0x7fa3556f8940> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa355e7bd30> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa355b92b70> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa35575ff60> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3556f8cc0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa355dbbe80> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa355b43908> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa355740898> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa35566a978> False\n",
            "<keras.layers.core.Activation object at 0x7fa355e962b0> False\n",
            "<keras.layers.core.Activation object at 0x7fa355bbeba8> False\n",
            "<keras.layers.core.Activation object at 0x7fa3556fea90> False\n",
            "<keras.layers.core.Activation object at 0x7fa3555ba9e8> False\n",
            "<keras.layers.merge.Concatenate object at 0x7fa3555e6f98> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa355390f98> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa3553f0470> False\n",
            "<keras.layers.core.Activation object at 0x7fa3552fbdd8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa35527bac8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa3552dcf60> False\n",
            "<keras.layers.core.Activation object at 0x7fa35529c828> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3555e6908> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3551e8908> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa355526a58> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa3551c7fd0> False\n",
            "<keras.layers.core.Activation object at 0x7fa355555240> False\n",
            "<keras.layers.core.Activation object at 0x7fa355186978> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3554d1198> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa35511ef60> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa355483588> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa355134e80> False\n",
            "<keras.layers.core.Activation object at 0x7fa355442be0> False\n",
            "<keras.layers.core.Activation object at 0x7fa3550f3898> False\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fa355020278> False\n",
            "<keras.layers.merge.Concatenate object at 0x7fa35506d3c8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa38637f0f0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa356574550> False\n",
            "<keras.layers.core.Activation object at 0x7fa354c4f6a0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa354f2ccf8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa354b7a160> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa354f0af28> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa354be2ac8> False\n",
            "<keras.layers.core.Activation object at 0x7fa354e9dda0> False\n",
            "<keras.layers.core.Activation object at 0x7fa354bb66d8> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa354e1a780> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa354db5630> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa354ae7f28> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3549bcfd0> False\n",
            "<keras.layers.pooling.AveragePooling2D object at 0x7fa35496cb70> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa35506d828> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa354e65c88> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa354d4ec18> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa354ab8e48> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa354949c88> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa354925668> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa354fdc7b8> False\n",
            "<keras.layers.core.Activation object at 0x7fa354db5cc0> False\n",
            "<keras.layers.core.Activation object at 0x7fa354cf39b0> False\n",
            "<keras.layers.core.Activation object at 0x7fa354a75f98> False\n",
            "<keras.layers.core.Activation object at 0x7fa3549a38d0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa354880748> False\n",
            "<keras.layers.core.Activation object at 0x7fa354fad550> False\n",
            "<keras.layers.merge.Concatenate object at 0x7fa38637f128> False\n",
            "<keras.layers.merge.Concatenate object at 0x7fa35496cfd0> False\n",
            "<keras.layers.core.Activation object at 0x7fa35485e4e0> False\n",
            "<keras.layers.merge.Concatenate object at 0x7fa354879a58> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa354492e10> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa354434e80> False\n",
            "<keras.layers.core.Activation object at 0x7fa354405fd0> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa354719198> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa35437e588> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa354704b00> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa35439ab38> False\n",
            "<keras.layers.core.Activation object at 0x7fa3546a8470> False\n",
            "<keras.layers.core.Activation object at 0x7fa354374978> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa354626390> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa354572e10> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3542ead68> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3541aada0> False\n",
            "<keras.layers.pooling.AveragePooling2D object at 0x7fa3540c8c18> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3548796d8> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa3545d7908> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa354534d68> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa354286a20> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa35418d588> False\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fa3540c8358> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa3547ccbe0> False\n",
            "<keras.layers.core.Activation object at 0x7fa35464fba8> False\n",
            "<keras.layers.core.Activation object at 0x7fa354540a90> False\n",
            "<keras.layers.core.Activation object at 0x7fa35425ed68> False\n",
            "<keras.layers.core.Activation object at 0x7fa35411cef0> False\n",
            "<keras.layers.normalization.BatchNormalization object at 0x7fa3540b9198> False\n",
            "<keras.layers.core.Activation object at 0x7fa35474b978> False\n",
            "<keras.layers.merge.Concatenate object at 0x7fa354492eb8> False\n",
            "<keras.layers.merge.Concatenate object at 0x7fa354099fd0> False\n",
            "<keras.layers.core.Activation object at 0x7fa3540367f0> False\n",
            "<keras.layers.merge.Concatenate object at 0x7fa353f83eb8> False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s9EaFDP5srBa"
      },
      "source": [
        "The model.summary() method call prints a summary of the NN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7ZKj8392nbgP",
        "outputId": "74e592be-1b40-4122-dec2-3177c34eed46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_v3 (Model)         (None, 8, 8, 2048)        21802784  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 131072)            0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 512)               67109376  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 88,912,673\n",
            "Trainable params: 67,109,889\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DmtkTn06pKxF"
      },
      "source": [
        "The \"output shape\" column shows how the size of your feature map evolves in each successive layer. The convolution layers reduce the size of the feature maps by a bit due to padding, and each pooling layer halves the dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PEkKSpZlvJXA"
      },
      "source": [
        "Next, we'll configure the specifications for model training. We will train our model with the `binary_crossentropy` loss, because it's a binary classification problem and our final activation is a sigmoid. (For a refresher on loss metrics, see the [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/descending-into-ml/video-lecture).) We will use the `rmsprop` optimizer with a learning rate of `0.001`. During training, we will want to monitor classification accuracy.\n",
        "\n",
        "**NOTE**: In this case, using the [RMSprop optimization algorithm](https://wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp) is preferable to [stochastic gradient descent](https://developers.google.com/machine-learning/glossary/#SGD) (SGD), because RMSprop automates learning-rate tuning for us. (Other optimizers, such as [Adam](https://wikipedia.org/wiki/Stochastic_gradient_descent#Adam) and [Adagrad](https://developers.google.com/machine-learning/glossary/#AdaGrad), also automatically adapt the learning rate during training, and would work equally well here.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8DHWhFP_uhq3",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2taMrQa3ehe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reduce_learning = callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=2,\n",
        "    verbose=1,\n",
        "    mode='auto',\n",
        "    min_delta=0.0001,\n",
        "    cooldown=2,\n",
        "    min_lr=0)\n",
        "\n",
        "eary_stopping = callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0,\n",
        "    patience=7,\n",
        "    verbose=1,\n",
        "    mode='auto')\n",
        "\n",
        "callbacks = [reduce_learning, eary_stopping]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mu3Jdwkjwax4"
      },
      "source": [
        "### Training\n",
        "Let's train for 15 epochs -- this may take a few minutes to run.\n",
        "\n",
        "Do note the values per epoch.\n",
        "\n",
        "The Loss and Accuracy are a great indication of progress of training. It's making a guess as to the classification of the training data, and then measuring it against the known label, calculating the result. Accuracy is the portion of correct guesses. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fb1_lgobv81m",
        "outputId": "cb105d72-ca32-4584-bab4-57b89abc4ed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=8,  \n",
        "      epochs=15,\n",
        "      verbose=1,\n",
        "      validation_data = validation_generator,\n",
        "      validation_steps=8,\n",
        "      callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "8/8 [==============================] - 34s 4s/step - loss: 7.0840 - acc: 0.4814 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 2/15\n",
            "8/8 [==============================] - 6s 781ms/step - loss: 7.4853 - acc: 0.5305 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 3/15\n",
            "8/8 [==============================] - 5s 684ms/step - loss: 7.4695 - acc: 0.5315 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 4/15\n",
            "8/8 [==============================] - 6s 692ms/step - loss: 7.5011 - acc: 0.5295 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 5/15\n",
            "8/8 [==============================] - 6s 699ms/step - loss: 7.9512 - acc: 0.5013 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 6/15\n",
            "8/8 [==============================] - 6s 752ms/step - loss: 8.8440 - acc: 0.4453 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 7/15\n",
            "8/8 [==============================] - 8s 952ms/step - loss: 7.4220 - acc: 0.5344 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 8/15\n",
            "8/8 [==============================] - 7s 924ms/step - loss: 6.9877 - acc: 0.5617 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 9/15\n",
            "8/8 [==============================] - 8s 954ms/step - loss: 7.4536 - acc: 0.5325 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 10/15\n",
            "8/8 [==============================] - 8s 1s/step - loss: 7.7688 - acc: 0.5127 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 11/15\n",
            "8/8 [==============================] - 8s 939ms/step - loss: 7.4062 - acc: 0.5354 - val_loss: 7.9712 - val_acc: 0.5000\n",
            "Epoch 00011: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A8xdDg3oJdm",
        "colab_type": "code",
        "outputId": "9001aa6b-9dc9-4ffc-e30c-a67ce0fac1c5",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(IMG_WIDTH, IMG_HEIGHT))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)/255.0\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a human\")\n",
        "  else:\n",
        "    print(fn + \" is a horse\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-07e013f7-526f-495f-adcc-6f80fde618bb\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-07e013f7-526f-495f-adcc-6f80fde618bb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}